---
title: "TODO: superstack agent"
parent: IoT Solutions Blog
description: TODO
image: /assets/images/blog/share-image.png
nav_order: 4
---

# **Superstack Agent**

Rohit Nareshkumar, Solutions Architect and Embedded Applications Engineer \| 28 July 2025
{: .float-left .fs-2 }

---

Why are LLMs useful for data analysis? They're not, unless you build the proper system to do it. What are they good for? Text compression, fuzzy search across other text, cross domain referencing. We leverage the power of these three in the right way to build our agent.

---

## How does the agent work? 

The superstack agent works through a series of orchestrated agents performing their specific tasks - filtering / fetching the relevant data, analysing the data and computing any parameters needed to answer the query, and finally summarising and framing the results in a helpful format. 

![block diagram showing the agent pipeline](/assets/images/blog/agent-pipeline.drawio.svg)

Let's examing these steps in detail with a simple example query for a greenhouse deployment - consider the user asks `what was the average temperature of my tomatoes yesterday?`

### 1. Filter
The role of this agent is to pick up on contextual information from the query and deployment metadata to filter the relevant data for the query. In the case of this example, that would entail inferring the device(s) referred to by 'tomato', the date-time range to apply based on the current date, and the relevant sensor data values required for the query.

### 2. Analyse
The role of this agent is to use the data to synthesise relevant metrics or parameters required to answer this question. In the case of this example, it would calculating the average of the temperature sensor(s), and handling missing or invalid data graciously.

### 3. Explain
The role of this agent is to summarize and synthesize the insights produced by the Analysis agent, to accurately answer the user query. In this simple case it would just be to present the average temperature, or to describe any issues encountered while computing this.

The key benefits of this approach are as follows
- Model agnostic: Having independently sequenced agents mean that we can easily swap out the underlying model for each particular step to the best performing, and cost efficient model. It also gives us the ability to quickly adopt and test the latest models.
- Data sovereignity: We have orchestrated the agents so that they do not have direct access to the data - they can only provide structured responses on how the data must be manipulated. These then run in a secure runtime on our servers, so the user data never leaves our servers.
- TODO: 1 more would be nice here

---

## GPT5 vs GPT4.1
While investigating the effectiveness of different models for the superstack agent we noticed some interesting behaviour.

The example deployment here consists of a number of devices in 2 groups - 'greenhouse' and 'outside'. Each device reports the temperature, soil moisture and light level in varying intervals.
One of the queries we tried was the following: `What was the temperature range in the greenhouse over the last 3 days?`, using GPT4.1 and then GPT5.
Both models were able to calculate the right answer, but with significantly different timelines. GPT4.1 took a total of 17.1 seconds, whereas GPT5 took 57.22 seconds! Comparing the intermediate steps and between runs, we noted that the code generated by GPT5 followed the instructions more closely, and was more meticulous, however the latency impact is tremendous. This tracks with openAI's docs other developer's experience, but our expectation was the internal model router mechanism of GPT5 would reroute simpler queries resulting in faster processing times.

Next we tried a much more complex query on a solar farm deployment consisting of 2 different panels 'farmhouse' and 'barn', which face different directions, and have simulated power output using real world solar incidence data. The query we ran was: `I am planning on installing another panel, which location is better? What percentage difference can I expect with a 15m^2 panel per day?`. We made sure the sampling times did not exactly match in the two sites, to keep it realistic - and this caused both models to struggle. GPT4.1 failed to give an answer, the filter agent was not able to infer a reasonable time range since it is not explicitly mentioned in the query. GPT5 correctly tried to fetch a year's worth of data, and came up with following plan:
```
- Aggregate each device’s measurements by day. 
- For every day and device, try to compute total daily energy in kWh using this priority: 
    (1) use explicit energy/yield counters (daily counter value or end-start difference with reset handling and unit detection for kWh/Wh), 
    (2) if no energy counters, integrate power over time within the day via trapezoidal approximation, 
    (3) as a last resort, integrate irradiance to Wh/m² and convert to electrical energy by assuming a typical module efficiency, then treat that as kWh per m². 
- Average daily energy across available days for each device, normalize by the device’s panel area to get kWh per m² per day, and compare locations. 
- Identify the better site as the one with higher per‑m² yield. Estimate the expected daily energy for a 15 m² panel at both sites and compute the percentage difference between them.
```
The result was the agent correctly picked the correct location after 191.23 seconds, although the computation had some parameters which were wrong, resulting in failure to compute the percentage difference. We didn't perform any GPT5 specific optimisations on the prompts to keep the comparison fair, and we expect the performance to significantly improve.

---

## Conclusion

TODO:
Performance vs time. Cost is the same so you don't need to mention it. Our future vision of how to improve the agent

future vision
- integrate multimodal outputs, charts etc.
- internally optimise agent prompts by model / vendor
- use the agent to create a data export bucket for further offline processing ?